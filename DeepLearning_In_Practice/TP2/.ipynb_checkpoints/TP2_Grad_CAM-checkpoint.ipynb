{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of CNN: Grad-CAM\n",
    "* **Objective**: Convolutional Neural Networks are widely used on computer vision. It is powerful for processing grid-like data. However we hardly know how and why it works, due to the lack of decomposability into individually intuitive components. In this assignment, we use Grad-CAM, which highlights the regions of the input image that were important for the neural network prediction.\n",
    "\n",
    "* **To be submitted by next session**: this notebook, **cleaned** (i.e. without results, for file size reasons: `menu > kernel > restart and clean`), in a state ready to be executed (if one just presses 'Enter' till the end, one should obtain all the results for all images) with a few comments at the end. No additional report, just the notebook!\n",
    "\n",
    "* NB: if `PIL` is not installed, try `conda install pillow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "We provide you a pretrained model `VGG-16` for `ImageNet` classification dataset.\n",
    "* **ImageNet**: A large dataset of photographs with 1 000 classes.\n",
    "* **VGG-16**: A deep architecture for image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vgg_16.png](https://www.researchgate.net/profile/Bibo_Shi/publication/323440752/figure/fig1/AS:739814685032448@1553396974148/The-architecture-of-VGG-16-model-To-represent-different-depth-levels-convolutional.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The downloading process may take a few minutes. \n",
    "vgg_model = models.vgg16(pretrained=True)# return the vgg-16 model pretrained on ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images\n",
    "We provide you 20 images from ImageNet (download link on the webpage of the course; notice that the images should be placed in a **sub**-directory of the path indicated below).<br>\n",
    "In order to use the pretrained model vgg-16, the input image should be normalized using `mean = [0.485, 0.456, 0.406]`, and `std = [0.229, 0.224, 0.225]`, and be resized as `(224, 224)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function of the input images\n",
    "def preprocess_image(dir_path):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = datasets.ImageFolder(dir_path, transforms.Compose([\n",
    "            transforms.Resize(256), \n",
    "            transforms.CenterCrop(224), # resize the image to 224x224\n",
    "            transforms.ToTensor(), # convert numpy.array to tensor\n",
    "            normalize])) #normalize the tensor\n",
    "\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images should be in a *sub*-folder of \"data/\" (ex: data/TP2_images/images.jpg) and *not* directly in \"data/\"!\n",
    "# otherwise the function won't find them\n",
    "dir_path = \"data/\" \n",
    "dataset = preprocess_image(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the orignal image \n",
    "index = 5\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the label of the input image, and print the top-3 possible classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = vgg_model(dataset[index][0].view(1, 3, 224, 224))\n",
    "\n",
    "values, indices = torch.topk(output, 3)\n",
    "print(\"Top 3-classes:\", indices[0].numpy())\n",
    "print(\"Raw class scores:\", values[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM \n",
    "* **Overview:** Given an image, and a category (‘tiger cat’) as input, we forward-propagate the image through the model to obtain the `raw class scores` before softmax. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the `rectified convolutional feature map` of interest, where we can compute the coarse Grad-CAM localization (blue heatmap).\n",
    "\n",
    "\n",
    "* **To Do**: Define your own function Grad_CAM to achieve the visualization of the given images. For each image, choose the top-3 possible labels as the desired classes. Compare the heatmaps of the three classes, and conclude. \n",
    "\n",
    "\n",
    "* **Hints**: \n",
    " + We need to record the output and grad_output of the feature maps to achieve Grad-CAM. In pytorch, the function `Hook` is defined for this purpose. Read the tutorial of [hook](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks) carefully. \n",
    " + The pretrained model vgg-16 doesn't have an activation function after its last layer, the output is indeed the `raw class scores`, you can use them directly. Run `print(vgg_model)` to get more information on VGG model.\n",
    " + The size of feature maps is 14x14, so your heatmap will have the same size. You need to project the heatmap to the resized image (224x224, not the original one, before the normalization) to have a better observation. The function [`torch.nn.functional.interpolate`](https://pytorch.org/docs/stable/nn.functional.html?highlight=interpolate#torch.nn.functional.interpolate) may help.  \n",
    " + Here is the link of the paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/pdf/1610.02391.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Grad_CAM](https://upload-images.jianshu.io/upload_images/415974-0147c44dcfb8cc1c.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layer where we want to extract the gradients from is the 29th one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its name is `features.29` and it is in the 31st position of the list `modules` built here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modules = []\n",
    "for name, module in vgg_model.named_modules():\n",
    "    modules.append(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the hooks that will store the values of the :\n",
    "- Activations : values that the last rectified convolutional feature map outputs during the forward pass\n",
    "- gradients : values that the last rectified convolutional feature map outputs during the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def forward_hook(m, input, output):\n",
    "     activation[m] = output\n",
    "gradient = {}\n",
    "def backward_hook(m, grad_in, grad_out):\n",
    "     gradient[m] = grad_out[0]\n",
    "        \n",
    "input_ = dataset[index][0].view(1, 3, 224, 224)\n",
    "modules[31].register_forward_hook(forward_hook)\n",
    "modules[31].register_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing a prediction, a forward pass will be executed and thus store the values in the dictionnary `activation` as we can see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.eval() # Ensure that the dropout layers are not used while predicting\n",
    "prediction = vgg_model(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the highest score outputed by the last layer (unnormalized) and then we back propagate it (partial derivatives). Thus the backward hook we placed before will retain the value of the gradient of this score wrt to the activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:,prediction.argmax(dim=1)].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation <==> variable  \n",
    "\n",
    "$y^c$ <==> Highest output score\n",
    "\n",
    "$A_{ij}^k$ <==> The activation of the pixel in the position $(i,j)$ of the $k$-th channel\n",
    "\n",
    "- The values contained in the `activation` dictionnary are the $A_{ij}^k$\n",
    "- The values contained in the `gradient` dictionnary are the $\\frac{\\partial y^c}{\\partial A_{ij}^k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (1) in the paper say we have to perform global average pooling on the gradient values : \n",
    "$$\\alpha_k^c = \\frac{1}{Z}\\sum_{ij} \\frac{\\partial y^c}{\\partial A_{ij}^k} $$  where $Z$ is a normalisation factor\n",
    "\n",
    "We thus obtain $k$ values one for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_gradient = torch.mean(gradient[modules[31]], dim=[0, 2, 3]) # equation number 1 in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we construct a linear combination of those coefficients $\\alpha_k^c$ and $A_{ij}^k$ sum them along the $k$ dimensions and only keep the positive factors of this sum. This is equation (2) in the paper.\n",
    "\n",
    "$$ L_{ij}^c =  ReLU (\\sum_k \\alpha_k^c A_{ij}^k ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_combination = torch.Tensor(activation[modules[31]][0])\n",
    "for chann in range(512) :\n",
    "    linear_combination[chann, : , :] *=  pooled_gradient[chann] # equation number 2 in the paper\n",
    "heatmap = nn.ReLU()(torch.sum(linear_combination, dim=0, keepdim=True)) # equation number 2 in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the pixels obtained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap.squeeze().detach().numpy());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to interpolate this $14x14$ picture with the original one after normalisation ($224x224$) in order to superimpose the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = torch.nn.functional.interpolate(heatmap.unsqueeze(0),\n",
    "                                       (224,224), \n",
    "                                       mode=\"bilinear\", \n",
    "                                       align_corners=False\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(input_.squeeze(0).permute(1,2,0).detach().numpy())\n",
    "plt.imshow(heat.squeeze().detach().numpy(), alpha=0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some area on the picture are highlighted ( in yellow/green) indicating the areas and pixels that influenced the most the output score of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap all this in this function : We give it an image and an input, and outputs the most influencing area of the top k prediction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRAD_Cam(input_, k):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    prediction = vgg_model(input_)\n",
    "    values, indices = torch.topk(prediction, 3)\n",
    "    \n",
    "    for i in range(k) :\n",
    "        \n",
    "        activation = {}\n",
    "        def forward_hook(m, input, output):\n",
    "             activation[m] = output\n",
    "        gradient = {}\n",
    "        def backward_hook(m, grad_in, grad_out):\n",
    "             gradient[m] = grad_out[0]\n",
    "\n",
    "        vgg_model.eval()\n",
    "        modules[31].register_forward_hook(forward_hook)\n",
    "        modules[31].register_backward_hook(backward_hook)\n",
    "        prediction = vgg_model(input_)\n",
    "        \n",
    "        prediction[:,prediction.argsort(descending=True)[:,i]].backward()\n",
    "\n",
    "        title = str(indices[0][i])\n",
    "        \n",
    "        pooled_gradient = torch.mean(gradient[modules[31]], dim=[0, 2, 3]) # equation number 1 in the paper\n",
    "\n",
    "        linear_combination = torch.Tensor(activation[modules[31]][0])\n",
    "        \n",
    "        for chann in range(512) :\n",
    "            linear_combination[chann, : , :] *=  pooled_gradient[chann] # equation number 2 in the paper\n",
    "            \n",
    "        heatmap = nn.ReLU()(torch.sum(linear_combination, dim=0, keepdim=True)) # equation number 2 in the paper\n",
    "        \n",
    "        heat = torch.nn.functional.interpolate(heatmap.unsqueeze(0),\n",
    "                                           (224,224), \n",
    "                                           mode=\"bilinear\", \n",
    "                                           align_corners=False\n",
    "                                           )\n",
    "        plt.subplot(1,k,i+1)\n",
    "#         plt.suptitle(title)\n",
    "        plt.imshow(input_.squeeze(0).permute(1,2,0).detach().numpy())\n",
    "        plt.imshow(heat.squeeze().detach().numpy(), alpha=0.85)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image);\n",
    "GRAD_Cam(dataset[index][0].view(1, 3, 224, 224), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image);\n",
    "GRAD_Cam(dataset[index][0].view(1, 3, 224, 224), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 19\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image);\n",
    "GRAD_Cam(dataset[index][0].view(1, 3, 224, 224), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 15\n",
    "input_image = Image.open(dataset.imgs[index][0]).convert('RGB')\n",
    "plt.imshow(input_image);\n",
    "GRAD_Cam(dataset[index][0].view(1, 3, 224, 224), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that sometimes, the model focuses on the same areas of the image to output the scores of the classes, but those scores are nevertheless reflecting its confidence on the class of interest ( the class we are backpropagating its score).\n",
    "\n",
    "We also see that the network is not totally independant from the background since it gives it a little bit of attention ( maybe it helps him being confident on the prediction he gives - we are less likely to see a cow in a bedroom- so maybe that is only a toy )."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
