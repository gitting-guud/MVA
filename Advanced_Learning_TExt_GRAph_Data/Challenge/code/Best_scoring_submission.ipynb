{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean_Alteg.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3XrOVNbn7u65",
        "kecYPVBW7w6r"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF1xFWw5kXJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install unidecode\n",
        "# !pip install transformers\n",
        "# !pip install stellargraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmt9OTCVolf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JWrs22Losbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cd '/content/gdrive/My Drive/Altegrad_Challenge/APPNP-master'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUkoT6XR1lln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d5473010-99ee-4924-eca6-9335a00ad13e"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import re\n",
        "import codecs\n",
        "import string\n",
        "import sys\n",
        "import nltk\n",
        "import torch\n",
        "import logging\n",
        "import time \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from ast import literal_eval\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from unidecode import unidecode\n",
        "from collections import Counter, OrderedDict\n",
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, \n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.metrics import accuracy_score, make_scorer\n",
        "\n",
        "\n",
        "from stellargraph.data import BiasedRandomWalk\n",
        "from stellargraph import StellarGraph, datasets\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
        "stop_words = set(stopwords.words('french') + stopwords.words('english'))\n",
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf_lJ2XE3KRi",
        "colab_type": "text"
      },
      "source": [
        "# PATHS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAFj7owd1yMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = \"./data/text/text/\"\n",
        "PATH_TEXT_TEXT = \"./data/text/text/\"\n",
        "EDGE_LIST_PATH = \"./data/edgelist.txt\"\n",
        "train_path = \"./data/train_noduplicates.csv\" \n",
        "test_path = \"./data/test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yly41fgK2UhN",
        "colab_type": "text"
      },
      "source": [
        "# HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHX81TRn_O3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Jan 29 09:33:54 2020\n",
        "\n",
        "@author: Houcine's laptop\n",
        "\"\"\"\n",
        "\n",
        "def build_graph():\n",
        "    '''Function that build a directed weighted graph from the edgelist.txt'''\n",
        "    G = nx.read_weighted_edgelist(EDGE_LIST_PATH, create_using=nx.DiGraph())\n",
        "    print(\"Number of nodes : \", G.number_of_nodes())\n",
        "    print(\"Number of edges : \", G.number_of_edges())\n",
        "    return G\n",
        "\n",
        "def build_train_test(train_path, test_path):\n",
        "    \"\"\"Function that reads the train.csv and returns the train Ids and train labels\n",
        "        and reads the test.csv and returns the test Ids\n",
        "    \"\"\"\n",
        "    with open(train_path, 'r') as f:\n",
        "        train_data = f.read().splitlines()\n",
        "        \n",
        "    train_hosts = list()\n",
        "    y_train = list()\n",
        "    for row in train_data:\n",
        "        host, label = row.split(\",\")\n",
        "        train_hosts.append(host)\n",
        "        y_train.append(label.lower())\n",
        "        \n",
        "    df_train = pd.DataFrame(data= y_train, index = train_hosts, columns= [\"class\"]).reset_index()\n",
        "    \n",
        "    with open(test_path, 'r') as f:\n",
        "        test_hosts = f.read().splitlines()\n",
        "    df_test =  pd.DataFrame(data=[] , index = test_hosts, columns= [\"class\"]).reset_index()\n",
        "    return df_train, df_test\n",
        "\n",
        "def write_submission(write_path, test_hosts, model_classes_list, predicted_probas):\n",
        "    \"\"\"Function that writes the submission file\n",
        "  there is a need to be pass it  : \n",
        "    - The path of the file to create\n",
        "    - The test Ids (returned by build_train_test)\n",
        "    - The classes labels as a list\n",
        "    - The predicted probas for those class labels (same order)\n",
        "    \"\"\"\n",
        "    with open(write_path, 'w') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter=',')\n",
        "        model_classes_list.insert(0, \"Host\")\n",
        "        writer.writerow(model_classes_list)\n",
        "        for i,test_host in enumerate(test_hosts):\n",
        "            lst = predicted_probas[i,:].tolist()\n",
        "            lst.insert(0, test_host)\n",
        "            writer.writerow(lst)\n",
        "\n",
        "def text_from_id(id):\n",
        "    id = str(id)\n",
        "    try :\n",
        "        with codecs.open(DATA_PATH+id, 'r', encoding=\"utf-8\") as f:\n",
        "            text = f.readlines()\n",
        "    except:\n",
        "        with codecs.open(DATA_PATH+id, 'r', encoding=\"latin-1\") as f:\n",
        "            text = f.readlines()\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "def build_local_test(train_hosts, y_train, size_local_test=.25):\n",
        "    \n",
        "    local_train, local_test, local_y_train, local_y_test = train_test_split(train_hosts, y_train,\n",
        "                                                                            stratify=y_train, \n",
        "                                                                            test_size=size_local_test)\n",
        "    \n",
        "    return local_train, local_y_train, local_test, local_y_test\n",
        "\n",
        "def compute_score(predictions, y_true, classes_order):\n",
        "    dico = {v:k for k, v in enumerate(classes_order)}\n",
        "    print(dico)\n",
        "    loss = 0\n",
        "    for i, cla in enumerate(y_true) :\n",
        "        loss -= np.log(predictions[i, dico[cla]])\n",
        "    loss = loss/len(y_true)\n",
        "    return loss\n",
        "\n",
        "def compute_score_3(predictions, y_true):\n",
        "    loss = 0\n",
        "    for i, cla in enumerate(y_true) :\n",
        "        loss -= np.log(predictions[i, cla])\n",
        "    loss = loss/len(y_true)\n",
        "    return loss\n",
        "\n",
        "def normalize_adjacency(A):\n",
        "    # Sets each component of the main diagonal of the adjacency matrix to 1\n",
        "    n = A.shape[0]\n",
        "    A = A + np.eye(n)\n",
        "\n",
        "    # Normalizes the emerging matrix such that each row sums to 1\n",
        "    D = np.sum(A, axis=1)\n",
        "    A_normalized = A/D\n",
        "\n",
        "    return A_normalized\n",
        "\n",
        "\n",
        "def loglikelihood_score(y_true, predictions, classes_order):\n",
        "    dico = {v:k for k, v in enumerate(classes_order)}\n",
        "    loss = 0\n",
        "    for i, cla in enumerate(y_true) :\n",
        "        loss -= np.log(predictions[i, dico[cla]])\n",
        "    loss = loss/len(y_true)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVD6m0OT2XQE",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrCa4miV_l0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_with_SEP(text):\n",
        "    \"\"\"text here is a list of sentences\"\"\"\n",
        "    return \" SEP \".join(text)\n",
        "\n",
        "def clean_page_from(page, rows_that_contain):\n",
        "    \"\"\"Removes the rows of 'page' that contain 'rows_that_contain' \n",
        "    \"\"\"\n",
        "    return [x for x in page if rows_that_contain not in x]\n",
        "\n",
        "def split_by_SEP(text) :\n",
        "    \"\"\"split again by SEP\"\"\"\n",
        "    return  text.split(\" SEP \")\n",
        "\n",
        "def remove_empty_rows(list_of_sentences):\n",
        "    list_ = [row.strip() for row in list_of_sentences]\n",
        "    return [row for row in list_ if len(row)>0]\n",
        "\n",
        "def process_text(text, start_fraction=0, end_fraction=1):\n",
        "    \"\"\"Read the text in but keeps the sentences in shape (do not split into tokens) \n",
        "    We only read the fraction of the text between start_fraction and end_fraction (we skip the headers of the website \n",
        "    and the contacts etc that are in the end\n",
        "    \n",
        "    While doing this applies this first preprocessing :\n",
        "        - removes accents\n",
        "        - removes the \"\\n\"\n",
        "        - removes the rows containing the at least one element of the list elements_discarding_the_row\n",
        "        - handling some weird cases where a website is split on two rows\n",
        "        - stripping extra spaces from the start and end of each sentence\n",
        "        - replace appostrophees by space (to delete stopwords further)\n",
        "    \"\"\"\n",
        "\n",
        "    text_ = [unidecode(sentence.replace(\"\\n\",\"\").lower()) for sentence in text[int(len(text)*start_fraction):int(len(text)*end_fraction)]]\n",
        "    \n",
        "    elements_discarding_the_row = [\"tel\", \"mail\", \"http\", \"www\", \"iframe\", \"button\", \n",
        "                               \"img\", \"submit\", \"lire la suite\", \"cedex\", \"html\", \"css\",\n",
        "                               \"cookies\",\"logo\", \"adresse electronique\",\"jpg\",\"jpeg\",\"png\",\"div\",\"alternate\",\".fr\",\".com\"]\n",
        "    \n",
        "    for e in elements_discarding_the_row :\n",
        "        text_ = clean_page_from(page=text_, rows_that_contain=e)\n",
        "    \n",
        "    text_ = [row for row in text_ if not ((\"&\" in row) & (\"=\" in row))] # fragmented website\n",
        "    text_ = [x.replace(\"'\",\" \").replace(\"’\",\" \") for x in text_ if x != \"\"]\n",
        "    text_ = [row.strip() for row in text_]\n",
        "    \n",
        "    return text_\n",
        "\n",
        "def replace_by_special_token(column_df) :\n",
        "    \n",
        "    col = column_df.str.replace(\"(\\d{1,2} [a-z]{3,9} \\d{4})|(\\d{1,2}\\s?/\\s?\\d{1,2}\\s?/\\s?\\d{4})\", \" date \")\n",
        "    col = column_df.str.replace(\"([a-z]{3,9} \\d{4})\", \" date \")\n",
        "    col = column_df.str.replace(\"(janvier|fevrier|mars|avril|mai|juin|juillet|aout|septembre|octobre|novembre|decembre) \", \" date \")\n",
        "    col = column_df.str.replace(\"(lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\", \" date \")\n",
        "    col = col.str.replace(\"\\d+\\s?(euro|euros|eur|EUR)\",\" prix \")\n",
        "    col = col.str.replace(\"\\d{1,2}(:|h)\\d{2}\",\" heure \")\n",
        "    return col\n",
        "\n",
        "def punctuation_by_space(column_df) :\n",
        "    \"\"\" column_df is a column of a dataframe\"\"\"\n",
        "    return column_df.str.replace(r\"[{}]\".format(string.punctuation+\"»\"), \" \")\n",
        "\n",
        "def remove_stop_words(text) :\n",
        "    return \" \".join([tok for tok in text.split(\" \") if tok not in stop_words])\n",
        "\n",
        "def remove_single_word_rows(text):\n",
        "    return [row for row in text if len(row.split())>1]\n",
        "\n",
        "def remove_single_characters(text):\n",
        "    return [' '.join( [w for w in row.split() if len(w)>2] ) for row in text]\n",
        "\n",
        "def filtering_most_repetitive_rows(text, L) :\n",
        "    \"\"\"Based on 4.2: sentence frequencies in EDA\n",
        "    L is the list of tokens to discard (see 4.2 on how it is built)\"\"\"\n",
        "    \n",
        "    for e in L :\n",
        "        text_ = clean_page_from(page=text, rows_that_contain=e)\n",
        "        \n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhamaGTk26t8",
        "colab_type": "text"
      },
      "source": [
        "# READING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-US4_Z7AWmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_hosts, test_hosts = build_train_test(train_path, test_path)\n",
        "train_hosts['text'] = train_hosts[\"index\"].apply(text_from_id)\n",
        "train_hosts[\"class_codes\"] = pd.Categorical(train_hosts[\"class\"]).codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0ZHabpmniNT",
        "colab_type": "text"
      },
      "source": [
        "If you run it in google drive : build the previous dataframes and save them, upload it to the drive and read it using this next cell (provided you mounter the Drive with the colab .ipynb and accessed to the corresponding directory )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL8z_AX4nhEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from ast import literal_eval\n",
        "# train_hosts = pd.read_csv(\"./data/train_hosts_UPLOAD_DRIVE.csv\", index_col=0)\n",
        "# train_hosts.text = train_hosts.text.apply(literal_eval)\n",
        "# test_hosts = pd.read_csv(\"./data/test_hosts_UPLOAD_DRIVE.csv\", index_col=0)\n",
        "# test_hosts.text = test_hosts.text.apply(literal_eval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGUcl_rX3XbN",
        "colab_type": "text"
      },
      "source": [
        "# Processing the data (There is no need to run these cells as we do not used the processed text but we put then here in order to see how to use the functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XrOVNbn7u65",
        "colab_type": "text"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaCAplCO3hLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_fraction = 0\n",
        "end_fraction = 1\n",
        "\n",
        "train_hosts[\"text_processed\"] = train_hosts.text.apply(process_text, args=(start_fraction, end_fraction,)) \n",
        "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(join_with_SEP)\n",
        "train_hosts[\"text_processed\"] = replace_by_special_token(train_hosts[\"text_processed\"])\n",
        "train_hosts[\"text_processed\"] = punctuation_by_space(train_hosts[\"text_processed\"])\n",
        "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(remove_stop_words)\n",
        "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(split_by_SEP)\n",
        "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(remove_empty_rows)\n",
        "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(remove_single_characters)\n",
        "train_hosts[\"text_processed_no_single_words\"] = train_hosts.text_processed.apply(remove_single_word_rows)\n",
        "train_hosts[\"text_processed_no_dupl\"] = train_hosts.text_processed.apply(lambda x : list(OrderedDict.fromkeys(x)))\n",
        "\n",
        "dico = dict()\n",
        "for cla in train_hosts[\"class\"].unique() :\n",
        "    dico[cla] = Counter(dict())\n",
        "    df = train_hosts[train_hosts[\"class\"] == cla]\n",
        "    for i in range(df.shape[0]) :\n",
        "        dico[cla] += Counter(df.text_processed_no_dupl.iloc[i])\n",
        "    # dico[cla] = dict(dico[cla])\n",
        "    dico[cla] = {k: v * df.shape[0]/train_hosts.shape[0] for k, v in dico[cla].items()}\n",
        "    dico[cla] = {k: v for k, v in sorted(dico[cla].items(), reverse=True, key=lambda item: item[1])}\n",
        "\n",
        "counter = Counter({})\n",
        "for cla in train_hosts[\"class\"].unique():\n",
        "    counter+= Counter(dico[cla])\n",
        "counter = {k: v for k, v in sorted(counter.items(), reverse=True, key=lambda item: item[1])}\n",
        "\n",
        "introduced_tokens = [\"date\", \"prix\", \"heure\"]\n",
        "LL = [k for k, v in counter.items() if v > 10]\n",
        "for tok in introduced_tokens :\n",
        "    if tok in LL :\n",
        "        LL.remove(tok)\n",
        "        \n",
        "train_hosts[\"text_processed_2\"] = train_hosts.text_processed_no_dupl.apply(filtering_most_repetitive_rows, args=(LL,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kecYPVBW7w6r",
        "colab_type": "text"
      },
      "source": [
        "## Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW49OYz57ygf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_hosts['text'] = test_hosts[\"index\"].apply(text_from_id)\n",
        "test_hosts[\"text_processed\"] = test_hosts.text.apply(process_text, args=(start_fraction, end_fraction,)) \n",
        "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(join_with_SEP)\n",
        "test_hosts[\"text_processed\"] = replace_by_special_token(test_hosts[\"text_processed\"])\n",
        "test_hosts[\"text_processed\"] = punctuation_by_space(test_hosts[\"text_processed\"])\n",
        "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(remove_stop_words)\n",
        "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(split_by_SEP)\n",
        "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(remove_empty_rows)\n",
        "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(remove_single_characters)\n",
        "test_hosts[\"text_processed_no_single_words\"] = test_hosts.text_processed.apply(remove_single_word_rows)\n",
        "test_hosts[\"text_processed_no_dupl\"] = test_hosts.text_processed.apply(lambda x : list(OrderedDict.fromkeys(x)))\n",
        "test_hosts[\"text_processed_2\"] = test_hosts.text_processed_no_dupl.apply(filtering_most_repetitive_rows, args=(LL,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9JYj-0E3zlL",
        "colab_type": "text"
      },
      "source": [
        "# Splitting training data into : local train/ local test (validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GmU2Xh83zQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "local_train, local_test = train_test_split(train_hosts, random_state=55)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK11YmS537fu",
        "colab_type": "text"
      },
      "source": [
        "# Best scoring approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnfhbyIhAlD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_ = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "model = CamembertModel.from_pretrained('camembert-base')\n",
        "model.eval();\n",
        "model.to('cuda');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3oBk7tJ40y-",
        "colab_type": "text"
      },
      "source": [
        "## Building BERT features (Using GPU and emptying cache and variables to clear memory space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFjnfP2R5DKt",
        "colab_type": "text"
      },
      "source": [
        "This was not executed locally as we do not dispose of enought memory space but we launched it on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDktfP8E5iaQ",
        "colab_type": "text"
      },
      "source": [
        "### Local training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTZOz3su4wwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEN = local_train.shape[0]\n",
        "for j in range(local_train.shape[0]):\n",
        "    sys.stdout.write('\\r'+str(j)+\"/\"+str(LEN))\n",
        "    cla = local_train[\"class_codes\"].iloc[j]\n",
        "\n",
        "    txt = \". \".join(local_train.text.iloc[j])\n",
        "    try :\n",
        "      tokens = tokenizer_.encode(txt, add_special_tokens=True)\n",
        "      SHAPE = len(tokens[1:-1])\n",
        "      new_tokens = []\n",
        "      for i in range(int(SHAPE/510)+1):\n",
        "          min_ = min((i+1)*510,SHAPE)\n",
        "          if min_ == SHAPE :\n",
        "              L = [tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id]\n",
        "              new_tokens.append(L + [tokenizer_.pad_token_id]*(512 - len(L)))\n",
        "          else :\n",
        "              new_tokens.append([tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id] )\n",
        "      # new_tokens = new_tokens[:350]\n",
        "      with torch.no_grad() :\n",
        "          new_train_ = model(torch.tensor(new_tokens).cuda())[0][:,0,:]\n",
        "      del new_tokens\n",
        "      torch.cuda.empty_cache()\n",
        "      if j == 0 :\n",
        "        new_train = new_train_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)\n",
        "        new_train_target = [cla]\n",
        "      else :\n",
        "        new_train = np.concatenate((new_train, \n",
        "                                    new_train_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)), \n",
        "                                   axis=0)\n",
        "        new_train_target.append(cla)\n",
        "    except :\n",
        "      new_train = np.concatenate((new_train, np.zeros((1,768))), axis=0)\n",
        "      new_train_target.extend([cla])\n",
        "new_train = np.array(new_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm4FdZu25khC",
        "colab_type": "text"
      },
      "source": [
        "### Local testing set (validation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKypl0eC4wuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEN = local_test.shape[0]\n",
        "for j in range(local_test.shape[0]):\n",
        "    sys.stdout.write('\\r'+str(j)+\"/\"+str(LEN))\n",
        "    cla = local_test[\"class_codes\"].iloc[j]\n",
        "\n",
        "    txt = \". \".join(local_test.text.iloc[j])\n",
        "    try :\n",
        "      tokens = tokenizer_.encode(txt, add_special_tokens=True)\n",
        "      SHAPE = len(tokens[1:-1])\n",
        "      new_tokens = []\n",
        "      for i in range(int(SHAPE/510)+1):\n",
        "          min_ = min((i+1)*510,SHAPE)\n",
        "          if min_ == SHAPE :\n",
        "              L = [tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id]\n",
        "              new_tokens.append(L + [tokenizer_.pad_token_id]*(512 - len(L)))\n",
        "          else :\n",
        "              new_tokens.append([tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id] )\n",
        "      # new_tokens = new_tokens[:350]\n",
        "      with torch.no_grad() :\n",
        "          new_test_ = model(torch.tensor(new_tokens).cuda())[0][:,0,:]\n",
        "      del new_tokens\n",
        "      torch.cuda.empty_cache()\n",
        "      if j == 0 :\n",
        "        new_test = new_test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)\n",
        "        new_test_target = [cla]\n",
        "      else :\n",
        "        new_test = np.concatenate((new_test, \n",
        "                                    new_test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)), \n",
        "                                   axis=0)\n",
        "        new_test_target.append(cla)\n",
        "    except :\n",
        "      new_test = np.concatenate((new_test, np.zeros((1,768))), axis=0)\n",
        "      new_test_target.extend([cla])\n",
        "new_test = np.array(new_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RagDcbTb60i_",
        "colab_type": "text"
      },
      "source": [
        "### Testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf8Mw9YJ6096",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEN = test_hosts.shape[0]\n",
        "exceptions = []\n",
        "for j in range(test_hosts.shape[0]):\n",
        "    sys.stdout.write('\\r'+str(j)+\"/\"+str(LEN))\n",
        "\n",
        "    txt = \". \".join(test_hosts.text_processed_no_dupl.iloc[j])\n",
        "    try :\n",
        "      tokens = tokenizer_.encode(txt, add_special_tokens=True)\n",
        "      SHAPE = len(tokens[1:-1])\n",
        "      new_tokens = []\n",
        "      for i in range(int(SHAPE/510)+1):\n",
        "          min_ = min((i+1)*510,SHAPE)\n",
        "          if min_ == SHAPE :\n",
        "              L = [tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id]\n",
        "              new_tokens.append(L + [tokenizer_.pad_token_id]*(512 - len(L)))\n",
        "          else :\n",
        "              new_tokens.append([tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id] )\n",
        "      # new_tokens = new_tokens[:300]\n",
        "      with torch.no_grad() :\n",
        "          test_ = model(torch.tensor(new_tokens).cuda())[0][:,0,:]\n",
        "      del new_tokens\n",
        "      torch.cuda.empty_cache()\n",
        "      if j == 0 :\n",
        "        test = test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)\n",
        "      else :\n",
        "        test = np.concatenate((test, test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)), axis=0)\n",
        "    except :\n",
        "        test = np.concatenate((test, np.zeros((1,768))), axis=0)\n",
        "        exceptions.append(j)\n",
        "\n",
        "test = np.array(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jew6HE1R_VjY",
        "colab_type": "text"
      },
      "source": [
        "### You can also read them as we already save them into .npy objects for further uses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoGXspcQ_VWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_train = np.load(\"./data/local_train_text_non_processed_BERT.npy\")\n",
        "# new_test = np.load(\"./data/local_test_text_non_processed_BERT.npy\")\n",
        "# new_train_target = np.load(\"./data/target_train_text_non_processed_BERT.npy\")\n",
        "# new_test_target = np.load(\"./data/target_local_test_text_non_processed_BERT.npy\")\n",
        "# test = np.load(\"./data/BERT_test_text_non_processed_76_133_289_304_349_437_525.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_BRGuS85vPI",
        "colab_type": "text"
      },
      "source": [
        "## Building the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQmn5qNJ4wqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoints = ModelCheckpoint('./data/weights.hdf5', monitor=\"val_loss\", mode=\"min\", verbose=True, save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=0.000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsH7Yvkb6CE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(embed_size=768, loss='categorical_crossentropy', do_rate=0.3, activ=\"relu\"):\n",
        "    inp    = Input(shape=(embed_size,))\n",
        "\n",
        "    # dense  = Dense(512, activation=activ)(inp)\n",
        "\n",
        "    output = Dense(8, activation=\"softmax\")(inp)\n",
        "    \n",
        "    model_ = Model(inputs=inp, outputs=output)\n",
        "    model_.compile(loss=loss, optimizer=Adam(lr=0.0001))\n",
        "    return model_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QuYExps4wmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 1000\n",
        "do_rate=0.1\n",
        "activ = \"tanh\"\n",
        "NN_clf = make_model(do_rate=do_rate, activ= activ)\n",
        "NN_clf.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRqODV2S6PPt",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the classifier on : the local train data and validating on the local test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuX1d7mk4wlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapping_labels = {lab:code for code, lab in enumerate(np.unique(new_train_target))}\n",
        "y_train_coded = [mapping_labels[lab] for lab in new_train_target]\n",
        "y_val_coded = [mapping_labels[lab] for lab in new_test_target]\n",
        "history = NN_clf.fit(new_train, to_categorical(y_train_coded), batch_size=batch_size, epochs=epochs, \n",
        "                    validation_data=[new_test, to_categorical(y_val_coded)], \n",
        "                     callbacks=[\n",
        "                                # reduce_lr,\n",
        "                                checkpoints ]\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhOJXduv67Iq",
        "colab_type": "text"
      },
      "source": [
        "### Refitting the model using the hyperparameters found during the previous step (validation) on the whole training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVMAC-K_4wh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "whole_train = np.concatenate((new_train, new_test),axis=0)\n",
        "whole_train_target = np.concatenate((new_train_target, new_test_target))\n",
        "\n",
        "epochs = 450\n",
        "\n",
        "NN_clf = make_model(do_rate=do_rate, activ= activ)\n",
        "\n",
        "mapping_labels = {lab:code for code, lab in enumerate(np.unique(whole_train_target))}\n",
        "y_train_coded = [mapping_labels[lab] for lab in whole_train_target]\n",
        "\n",
        "history = NN_clf.fit(x=whole_train,\n",
        "                     y=to_categorical(y_train_coded), \n",
        "                     batch_size=batch_size, \n",
        "                     epochs=epochs, \n",
        "                     callbacks=[\n",
        "                                checkpoints\n",
        "                                ]\n",
        "                     )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9HiI0J87VvT",
        "colab_type": "text"
      },
      "source": [
        "### Saving the model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chv5tzU77Y7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NN_clf.save(\"./data/model_wrap_up_20_HK_Approach3_rawtext_NN_CLF.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXCbQoER7JJg",
        "colab_type": "text"
      },
      "source": [
        "## Prediction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCElU89qOja",
        "colab_type": "text"
      },
      "source": [
        "### Predict using the BERT features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXvykI-Y7Iwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = NN_clf.predict(test)\n",
        "write_submission(\"./data/wrap_up_20_HK_Approach3_rawtext_NN_CLF.csv\", \n",
        "                 list(test_hosts[\"index\"]), \n",
        "                 model_classes_list=list(np.array(train_hosts[[\"class\",\"class_codes\"]].drop_duplicates().sort_values(by='class_codes'))[:,0]), \n",
        "                 predicted_probas=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uetDj4Ts0ft",
        "colab_type": "text"
      },
      "source": [
        "### Replacing the empty texts predictions using Node2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2n8hmB6qCxd",
        "colab_type": "text"
      },
      "source": [
        "#### Building the graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htTx7mBOszqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a graph with all the nodes: \n",
        "G = build_graph()\n",
        "\n",
        "Subgraph = G.subgraph(train_hosts['index'].tolist() + test_hosts['index'].tolist())\n",
        "print(\"Number of nodes : \", Subgraph.number_of_nodes())\n",
        "print(\"Number of edges : \", Subgraph.number_of_edges())\n",
        "\n",
        "Graph_stellar = StellarGraph.from_networkx(G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFDnx3nOqhto",
        "colab_type": "text"
      },
      "source": [
        "#### Launch the random walk (This takes nearly 2 hours to finish) this is why we saved the obtained features into a .csv and extracting out the features of the nodes we are interested in (trains_hosts and test_hosts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOlUfbqSvsYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Launching Biased Random walk on the whole graph :\n",
        "\n",
        "rw = BiasedRandomWalk(Graph_stellar)\n",
        "\n",
        "time_ = time.time()\n",
        "walks = rw.run(\n",
        "    nodes=list(Subgraph.nodes()),  # root nodes\n",
        "    length=100,  # maximum length of a random walk\n",
        "    n=15,  # number of random walks per root node\n",
        "    p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
        "    q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
        ")\n",
        "print(\"Number of random walks: {}, {:.2f}\".format(len(walks), time.time() - time_))\n",
        "str_walks = [[str(n) for n in walk] for walk in walks]\n",
        "\n",
        "model = Word2Vec(str_walks, size=128, window=5, min_count=0, sg=1, workers=4)\n",
        "\n",
        "node_ids = [node for node in model.wv.index2word if (node in train_hosts['index'].tolist() or node in test_hosts['index'].tolist())] # list of node IDs\n",
        "data_ids_train_test = pd.DataFrame(model.wv[node_ids])\n",
        "data_ids_train_test[\"node_id\"] = node_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng_mh-uAsW8q",
        "colab_type": "text"
      },
      "source": [
        "if you ever run the previous cell, it is advised to save the results we get for further use without having to rerun it again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkVIphvesUbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataFrame_all_nodes = pd.DataFrame(model.wv.vectors)\n",
        "# DataFrame_all_nodes['node_id'] = model.wv.index2word\n",
        "# DataFrame_all_nodes.to_csv('./data/DataFrame_all_nodes.csv', index=False)\n",
        "# data_ids_train_test.to_csv('./data/data_ids_train_test.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE8nCSeXww9U",
        "colab_type": "text"
      },
      "source": [
        "#### Constructing the Dataset :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEq0qj66wDbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_ids_train_test = pd.read_csv('./data/data_ids_train_test.csv')\n",
        "idx_train = pd.Series(data_ids_train_test.node_id.values.astype(str)).isin(train_hosts['index'].values)\n",
        "df_train = data_ids_train_test[idx_train]\n",
        "df_test = data_ids_train_test[~ idx_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Px7hlMw3K0",
        "colab_type": "text"
      },
      "source": [
        "#### Constucting the Classifier (Hyper parameter tuning with GridSearchCV):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob0X2zfWwkIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = df_train.iloc[:,:-1].values\n",
        "y_train =[train_hosts[train_hosts['index']==str(node)]['class'].iloc[0] for node in df_train['node_id'].values]\n",
        "\n",
        "grid={\"C\":np.logspace(-1,0,20)}\n",
        "\n",
        "logreg = LogisticRegression(solver='lbfgs',  multi_class='auto', max_iter=25000, n_jobs=-1)\n",
        "\n",
        "classes_order = LogisticRegression(solver='lbfgs',  multi_class='auto').fit(X_train[:, :2], y_train).classes_\n",
        "score_function = make_scorer(loglikelihood_score, greater_is_better=False, classes_order=classes_order, needs_proba=True)\n",
        "\n",
        "logreg_cv = GridSearchCV(logreg,grid,cv=3, verbose=3, n_jobs=-1, scoring=score_function)\n",
        "\n",
        "logreg_cv.fit(X_train, y_train)\n",
        "\n",
        "print(logreg_cv.best_params_)\n",
        "print('Score on the local test : ', logreg_cv.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3zfKzuMxMYL",
        "colab_type": "text"
      },
      "source": [
        "#### Prediction (This is what scores 1.14 on the LB ):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHivaw79xCjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = np.vstack([df_test[df_test.node_id == int(node)].iloc[:,:-1].values for node in test_hosts['index'].values])\n",
        "predictions = logreg_cv.best_estimator_.predict_proba(test_set)\n",
        "write_submission(\"./data/wrap_up_1000_node2vec.csv\", \n",
        "                 list(test_hosts[\"index\"]), \n",
        "                 model_classes_list=list(logreg_cv.best_estimator_.classes_), \n",
        "                 predicted_probas=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdTmkOBpxV1D",
        "colab_type": "text"
      },
      "source": [
        "#### Combining NN predictions and Graph predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLiLRjlKxQ3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neural_network_predictions = pd.read_csv(\"./data/wrap_up_20_HK_Approach3_rawtext_NN_CLF.csv\")\n",
        "node2vec_predictions = pd.read_csv(\"./data/wrap_up_1000_node2vec.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heWk5czXxjSR",
        "colab_type": "text"
      },
      "source": [
        "#### Extracting empty test set texts (under 80 characters) and replacing the probability vectors with those we got by the LogReg fit on the Node2vec features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U4RlbLBxhdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = test_hosts.text.apply(lambda x:len(x))\n",
        "test_hosts[alpha < 80].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioKBcRtExgrZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neural_network_predictions.iloc[test_hosts[alpha < 80].index] = node2vec_predictions.iloc[test_hosts[alpha < 80].index]\n",
        "neural_network_predictions.to_csv('./data/wrap_up_24000_merge_NN_Node2vec.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}