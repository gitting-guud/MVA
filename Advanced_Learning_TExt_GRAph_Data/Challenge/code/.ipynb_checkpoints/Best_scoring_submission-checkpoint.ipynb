{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUkoT6XR1lln"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "import sys\n",
    "import nltk\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from unidecode import unidecode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter, OrderedDict\n",
    "from transformers import CamembertTokenizer, CamembertModel, CamembertForSequenceClassification\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Bidirectional, LSTM, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Input, Dropout\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "nltk.download('stopwords')\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "stop_words = set(stopwords.words('french') + stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vf_lJ2XE3KRi"
   },
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAFj7owd1yMS"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/text/text/\"\n",
    "PATH_TEXT_TEXT = \"./data/text/text/\"\n",
    "EDGE_LIST_PATH = \"./data/edgelist.txt\"\n",
    "train_path = \"./data/train_noduplicates.csv\" \n",
    "test_path = \"./data/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yly41fgK2UhN"
   },
   "source": [
    "# HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "MHX81TRn_O3G",
    "outputId": "7448e428-6ea2-4746-e8b7-e5ddd8d0e142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jan 29 09:33:54 2020\n",
    "\n",
    "@author: Houcine's laptop\n",
    "\"\"\"\n",
    "\n",
    "def build_graph():\n",
    "    '''Function that build a directed weighted graph from the edgelist.txt'''\n",
    "    G = nx.read_weighted_edgelist(EDGE_LIST_PATH, create_using=nx.DiGraph())\n",
    "    print(\"Number of nodes : \", G.number_of_nodes())\n",
    "    print(\"Number of edges : \", G.number_of_edges())\n",
    "    return G\n",
    "\n",
    "def build_train_test(train_path, test_path):\n",
    "    \"\"\"Function that reads the train.csv and returns the train Ids and train labels\n",
    "        and reads the test.csv and returns the test Ids\n",
    "    \"\"\"\n",
    "    with open(train_path, 'r') as f:\n",
    "        train_data = f.read().splitlines()\n",
    "        \n",
    "    train_hosts = list()\n",
    "    y_train = list()\n",
    "    for row in train_data:\n",
    "        host, label = row.split(\",\")\n",
    "        train_hosts.append(host)\n",
    "        y_train.append(label.lower())\n",
    "        \n",
    "    df_train = pd.DataFrame(data= y_train, index = train_hosts, columns= [\"class\"]).reset_index()\n",
    "    \n",
    "    with open(test_path, 'r') as f:\n",
    "        test_hosts = f.read().splitlines()\n",
    "    df_test =  pd.DataFrame(data=[] , index = test_hosts, columns= [\"class\"]).reset_index()\n",
    "    return df_train, df_test\n",
    "\n",
    "def write_submission(write_path, test_hosts, model_classes_list, predicted_probas):\n",
    "    \"\"\"Function that writes the submission file\n",
    "  there is a need to be pass it  : \n",
    "    - The path of the file to create\n",
    "    - The test Ids (returned by build_train_test)\n",
    "    - The classes labels as a list\n",
    "    - The predicted probas for those class labels (same order)\n",
    "    \"\"\"\n",
    "    with open(write_path, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        model_classes_list.insert(0, \"Host\")\n",
    "        writer.writerow(model_classes_list)\n",
    "        for i,test_host in enumerate(test_hosts):\n",
    "            lst = predicted_probas[i,:].tolist()\n",
    "            lst.insert(0, test_host)\n",
    "            writer.writerow(lst)\n",
    "\n",
    "def text_from_id(id):\n",
    "    id = str(id)\n",
    "    try :\n",
    "        with codecs.open(DATA_PATH+id, 'r', encoding=\"utf-8\") as f:\n",
    "            text = f.readlines()\n",
    "    except:\n",
    "        with codecs.open(DATA_PATH+id, 'r', encoding=\"latin-1\") as f:\n",
    "            text = f.readlines()\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def build_local_test(train_hosts, y_train, size_local_test=.25):\n",
    "    \n",
    "    local_train, local_test, local_y_train, local_y_test = train_test_split(train_hosts, y_train,\n",
    "                                                                            stratify=y_train, \n",
    "                                                                            test_size=size_local_test)\n",
    "    \n",
    "    return local_train, local_y_train, local_test, local_y_test\n",
    "\n",
    "def compute_score(predictions, y_true, classes_order):\n",
    "    dico = {v:k for k, v in enumerate(classes_order)}\n",
    "    print(dico)\n",
    "    loss = 0\n",
    "    for i, cla in enumerate(y_true) :\n",
    "        loss -= np.log(predictions[i, dico[cla]])\n",
    "    loss = loss/len(y_true)\n",
    "    return loss\n",
    "\n",
    "def compute_score_3(predictions, y_true):\n",
    "    loss = 0\n",
    "    for i, cla in enumerate(y_true) :\n",
    "        loss -= np.log(predictions[i, cla])\n",
    "    loss = loss/len(y_true)\n",
    "    return loss\n",
    "\n",
    "def normalize_adjacency(A):\n",
    "    # Sets each component of the main diagonal of the adjacency matrix to 1\n",
    "    n = A.shape[0]\n",
    "    A = A + np.eye(n)\n",
    "\n",
    "    # Normalizes the emerging matrix such that each row sums to 1\n",
    "    D = np.sum(A, axis=1)\n",
    "    A_normalized = A/D\n",
    "\n",
    "    return A_normalized\n",
    "\n",
    "\n",
    "def loglikelihood_score(y_true, predictions, classes_order):\n",
    "    dico = {v:k for k, v in enumerate(classes_order)}\n",
    "    loss = 0\n",
    "    for i, cla in enumerate(y_true) :\n",
    "        loss -= np.log(predictions[i, dico[cla]])\n",
    "    loss = loss/len(y_true)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVD6m0OT2XQE"
   },
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrCa4miV_l0i"
   },
   "outputs": [],
   "source": [
    "def join_with_SEP(text):\n",
    "    \"\"\"text here is a list of sentences\"\"\"\n",
    "    return \" SEP \".join(text)\n",
    "\n",
    "def clean_page_from(page, rows_that_contain):\n",
    "    \"\"\"Removes the rows of 'page' that contain 'rows_that_contain' \n",
    "    \"\"\"\n",
    "    return [x for x in page if rows_that_contain not in x]\n",
    "\n",
    "def split_by_SEP(text) :\n",
    "    \"\"\"split again by SEP\"\"\"\n",
    "    return  text.split(\" SEP \")\n",
    "\n",
    "def remove_empty_rows(list_of_sentences):\n",
    "    list_ = [row.strip() for row in list_of_sentences]\n",
    "    return [row for row in list_ if len(row)>0]\n",
    "\n",
    "def process_text(text, start_fraction=0, end_fraction=1):\n",
    "    \"\"\"Read the text in but keeps the sentences in shape (do not split into tokens) \n",
    "    We only read the fraction of the text between start_fraction and end_fraction (we skip the headers of the website \n",
    "    and the contacts etc that are in the end\n",
    "    \n",
    "    While doing this applies this first preprocessing :\n",
    "        - removes accents\n",
    "        - removes the \"\\n\"\n",
    "        - removes the rows containing the at least one element of the list elements_discarding_the_row\n",
    "        - handling some weird cases where a website is split on two rows\n",
    "        - stripping extra spaces from the start and end of each sentence\n",
    "        - replace appostrophees by space (to delete stopwords further)\n",
    "    \"\"\"\n",
    "\n",
    "    text_ = [unidecode(sentence.replace(\"\\n\",\"\").lower()) for sentence in text[int(len(text)*start_fraction):int(len(text)*end_fraction)]]\n",
    "    \n",
    "    elements_discarding_the_row = [\"tel\", \"mail\", \"http\", \"www\", \"iframe\", \"button\", \n",
    "                               \"img\", \"submit\", \"lire la suite\", \"cedex\", \"html\", \"css\",\n",
    "                               \"cookies\",\"logo\", \"adresse electronique\",\"jpg\",\"jpeg\",\"png\",\"div\",\"alternate\",\".fr\",\".com\"]\n",
    "    \n",
    "    for e in elements_discarding_the_row :\n",
    "        text_ = clean_page_from(page=text_, rows_that_contain=e)\n",
    "    \n",
    "    text_ = [row for row in text_ if not ((\"&\" in row) & (\"=\" in row))] # fragmented website\n",
    "    text_ = [x.replace(\"'\",\" \").replace(\"’\",\" \") for x in text_ if x != \"\"]\n",
    "    text_ = [row.strip() for row in text_]\n",
    "    \n",
    "    return text_\n",
    "\n",
    "def replace_by_special_token(column_df) :\n",
    "    \n",
    "    col = column_df.str.replace(\"(\\d{1,2} [a-z]{3,9} \\d{4})|(\\d{1,2}\\s?/\\s?\\d{1,2}\\s?/\\s?\\d{4})\", \" date \")\n",
    "    col = column_df.str.replace(\"([a-z]{3,9} \\d{4})\", \" date \")\n",
    "    col = column_df.str.replace(\"(janvier|fevrier|mars|avril|mai|juin|juillet|aout|septembre|octobre|novembre|decembre) \", \" date \")\n",
    "    col = column_df.str.replace(\"(lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\", \" date \")\n",
    "    col = col.str.replace(\"\\d+\\s?(euro|euros|eur|EUR)\",\" prix \")\n",
    "    col = col.str.replace(\"\\d{1,2}(:|h)\\d{2}\",\" heure \")\n",
    "    return col\n",
    "\n",
    "def punctuation_by_space(column_df) :\n",
    "    \"\"\" column_df is a column of a dataframe\"\"\"\n",
    "    return column_df.str.replace(r\"[{}]\".format(string.punctuation+\"»\"), \" \")\n",
    "\n",
    "def remove_stop_words(text) :\n",
    "    return \" \".join([tok for tok in text.split(\" \") if tok not in stop_words])\n",
    "\n",
    "def remove_single_word_rows(text):\n",
    "    return [row for row in text if len(row.split())>1]\n",
    "\n",
    "def remove_single_characters(text):\n",
    "    return [' '.join( [w for w in row.split() if len(w)>2] ) for row in text]\n",
    "\n",
    "def filtering_most_repetitive_rows(text, L) :\n",
    "    \"\"\"Based on 4.2: sentence frequencies in EDA\n",
    "    L is the list of tokens to discard (see 4.2 on how it is built)\"\"\"\n",
    "    \n",
    "    for e in L :\n",
    "        text_ = clean_page_from(page=text, rows_that_contain=e)\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhamaGTk26t8"
   },
   "source": [
    "# READING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-US4_Z7AWmV"
   },
   "outputs": [],
   "source": [
    "train_hosts, test_hosts = build_train_test(train_path, test_path)\n",
    "train_hosts['text'] = train_hosts[\"index\"].apply(text_from_id)\n",
    "train_hosts[\"class_codes\"] = pd.Categorical(train_hosts[\"class\"]).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGUcl_rX3XbN"
   },
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XrOVNbn7u65"
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaCAplCO3hLb"
   },
   "outputs": [],
   "source": [
    "start_fraction = 0\n",
    "end_fraction = 1\n",
    "\n",
    "train_hosts[\"text_processed\"] = train_hosts.text.apply(process_text, args=(start_fraction, end_fraction,)) \n",
    "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(join_with_SEP)\n",
    "train_hosts[\"text_processed\"] = replace_by_special_token(train_hosts[\"text_processed\"])\n",
    "train_hosts[\"text_processed\"] = punctuation_by_space(train_hosts[\"text_processed\"])\n",
    "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(remove_stop_words)\n",
    "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(split_by_SEP)\n",
    "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(remove_empty_rows)\n",
    "train_hosts[\"text_processed\"] = train_hosts.text_processed.apply(remove_single_characters)\n",
    "train_hosts[\"text_processed_no_single_words\"] = train_hosts.text_processed.apply(remove_single_word_rows)\n",
    "train_hosts[\"text_processed_no_dupl\"] = train_hosts.text_processed.apply(lambda x : list(OrderedDict.fromkeys(x)))\n",
    "\n",
    "dico = dict()\n",
    "for cla in train_hosts[\"class\"].unique() :\n",
    "    dico[cla] = Counter(dict())\n",
    "    df = train_hosts[train_hosts[\"class\"] == cla]\n",
    "    for i in range(df.shape[0]) :\n",
    "        dico[cla] += Counter(df.text_processed_no_dupl.iloc[i])\n",
    "    # dico[cla] = dict(dico[cla])\n",
    "    dico[cla] = {k: v * df.shape[0]/train_hosts.shape[0] for k, v in dico[cla].items()}\n",
    "    dico[cla] = {k: v for k, v in sorted(dico[cla].items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "counter = Counter({})\n",
    "for cla in train_hosts[\"class\"].unique():\n",
    "    counter+= Counter(dico[cla])\n",
    "counter = {k: v for k, v in sorted(counter.items(), reverse=True, key=lambda item: item[1])}\n",
    "\n",
    "introduced_tokens = [\"date\", \"prix\", \"heure\"]\n",
    "LL = [k for k, v in counter.items() if v > 10]\n",
    "for tok in introduced_tokens :\n",
    "    if tok in LL :\n",
    "        LL.remove(tok)\n",
    "        \n",
    "train_hosts[\"text_processed_2\"] = train_hosts.text_processed_no_dupl.apply(filtering_most_repetitive_rows, args=(LL,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kecYPVBW7w6r"
   },
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZW49OYz57ygf"
   },
   "outputs": [],
   "source": [
    "test_hosts['text'] = test_hosts[\"index\"].apply(text_from_id)\n",
    "test_hosts[\"text_processed\"] = test_hosts.text.apply(process_text, args=(start_fraction, end_fraction,)) \n",
    "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(join_with_SEP)\n",
    "test_hosts[\"text_processed\"] = replace_by_special_token(test_hosts[\"text_processed\"])\n",
    "test_hosts[\"text_processed\"] = punctuation_by_space(test_hosts[\"text_processed\"])\n",
    "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(remove_stop_words)\n",
    "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(split_by_SEP)\n",
    "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(remove_empty_rows)\n",
    "test_hosts[\"text_processed\"] = test_hosts.text_processed.apply(remove_single_characters)\n",
    "test_hosts[\"text_processed_no_single_words\"] = test_hosts.text_processed.apply(remove_single_word_rows)\n",
    "test_hosts[\"text_processed_no_dupl\"] = test_hosts.text_processed.apply(lambda x : list(OrderedDict.fromkeys(x)))\n",
    "test_hosts[\"text_processed_2\"] = test_hosts.text_processed_no_dupl.apply(filtering_most_repetitive_rows, args=(LL,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9JYj-0E3zlL"
   },
   "source": [
    "# Splitting training data into : local train/ local test (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GmU2Xh83zQs"
   },
   "outputs": [],
   "source": [
    "local_train, local_test = train_test_split(train_hosts, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FK11YmS537fu"
   },
   "source": [
    "# Best scoring approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnfhbyIhAlD7"
   },
   "outputs": [],
   "source": [
    "tokenizer_ = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "model = CamembertModel.from_pretrained('camembert-base')\n",
    "model.eval();\n",
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3oBk7tJ40y-"
   },
   "source": [
    "## Building BERT features (Using GPU and emptying cache and variables to clear memory space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFjnfP2R5DKt"
   },
   "source": [
    "This was not executed locally as we do not dispose of enought memory space but we launched it on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDktfP8E5iaQ"
   },
   "source": [
    "### Local training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTZOz3su4wwC"
   },
   "outputs": [],
   "source": [
    "LEN = local_train.shape[0]\n",
    "for j in range(local_train.shape[0]):\n",
    "    sys.stdout.write('\\r'+str(j)+\"/\"+str(LEN))\n",
    "    cla = local_train[\"class_codes\"].iloc[j]\n",
    "\n",
    "    txt = \". \".join(local_train.text.iloc[j])\n",
    "    try :\n",
    "      tokens = tokenizer_.encode(txt, add_special_tokens=True)\n",
    "      SHAPE = len(tokens[1:-1])\n",
    "      new_tokens = []\n",
    "      for i in range(int(SHAPE/510)+1):\n",
    "          min_ = min((i+1)*510,SHAPE)\n",
    "          if min_ == SHAPE :\n",
    "              L = [tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id]\n",
    "              new_tokens.append(L + [tokenizer_.pad_token_id]*(512 - len(L)))\n",
    "          else :\n",
    "              new_tokens.append([tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id] )\n",
    "      # new_tokens = new_tokens[:350]\n",
    "      with torch.no_grad() :\n",
    "          new_train_ = model(torch.tensor(new_tokens).cuda())[0][:,0,:]\n",
    "      del new_tokens\n",
    "      torch.cuda.empty_cache()\n",
    "      if j == 0 :\n",
    "        new_train = new_train_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)\n",
    "        new_train_target = [cla]\n",
    "      else :\n",
    "        new_train = np.concatenate((new_train, \n",
    "                                    new_train_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)), \n",
    "                                   axis=0)\n",
    "        new_train_target.append(cla)\n",
    "    except :\n",
    "      new_train = np.concatenate((new_train, np.zeros((1,768))), axis=0)\n",
    "      new_train_target.extend([cla])\n",
    "new_train = np.array(new_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zm4FdZu25khC"
   },
   "source": [
    "### Local testing set (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKypl0eC4wuV"
   },
   "outputs": [],
   "source": [
    "LEN = local_test.shape[0]\n",
    "for j in range(local_test.shape[0]):\n",
    "    sys.stdout.write('\\r'+str(j)+\"/\"+str(LEN))\n",
    "    cla = local_test[\"class_codes\"].iloc[j]\n",
    "\n",
    "    txt = \". \".join(local_test.text.iloc[j])\n",
    "    try :\n",
    "      tokens = tokenizer_.encode(txt, add_special_tokens=True)\n",
    "      SHAPE = len(tokens[1:-1])\n",
    "      new_tokens = []\n",
    "      for i in range(int(SHAPE/510)+1):\n",
    "          min_ = min((i+1)*510,SHAPE)\n",
    "          if min_ == SHAPE :\n",
    "              L = [tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id]\n",
    "              new_tokens.append(L + [tokenizer_.pad_token_id]*(512 - len(L)))\n",
    "          else :\n",
    "              new_tokens.append([tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id] )\n",
    "      # new_tokens = new_tokens[:350]\n",
    "      with torch.no_grad() :\n",
    "          new_test_ = model(torch.tensor(new_tokens).cuda())[0][:,0,:]\n",
    "      del new_tokens\n",
    "      torch.cuda.empty_cache()\n",
    "      if j == 0 :\n",
    "        new_test = new_test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)\n",
    "        new_test_target = [cla]\n",
    "      else :\n",
    "        new_test = np.concatenate((new_test, \n",
    "                                    new_test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)), \n",
    "                                   axis=0)\n",
    "        new_test_target.append(cla)\n",
    "    except :\n",
    "      new_test = np.concatenate((new_test, np.zeros((1,768))), axis=0)\n",
    "      new_test_target.extend([cla])\n",
    "new_test = np.array(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RagDcbTb60i_"
   },
   "source": [
    "### Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cf8Mw9YJ6096"
   },
   "outputs": [],
   "source": [
    "LEN = test_hosts.shape[0]\n",
    "exceptions = []\n",
    "for j in range(test_hosts.shape[0]):\n",
    "    sys.stdout.write('\\r'+str(j)+\"/\"+str(LEN))\n",
    "\n",
    "    txt = \". \".join(test_hosts.text_processed_no_dupl.iloc[j])\n",
    "    try :\n",
    "      tokens = tokenizer_.encode(txt, add_special_tokens=True)\n",
    "      SHAPE = len(tokens[1:-1])\n",
    "      new_tokens = []\n",
    "      for i in range(int(SHAPE/510)+1):\n",
    "          min_ = min((i+1)*510,SHAPE)\n",
    "          if min_ == SHAPE :\n",
    "              L = [tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id]\n",
    "              new_tokens.append(L + [tokenizer_.pad_token_id]*(512 - len(L)))\n",
    "          else :\n",
    "              new_tokens.append([tokenizer_.cls_token_id] + tokens[i*510:min_] + [tokenizer_.eos_token_id] )\n",
    "      # new_tokens = new_tokens[:300]\n",
    "      with torch.no_grad() :\n",
    "          test_ = model(torch.tensor(new_tokens).cuda())[0][:,0,:]\n",
    "      del new_tokens\n",
    "      torch.cuda.empty_cache()\n",
    "      if j == 0 :\n",
    "        test = test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)\n",
    "      else :\n",
    "        test = np.concatenate((test, test_.detach().cpu().numpy().mean(axis=0).reshape(1,-1)), axis=0)\n",
    "    except :\n",
    "        test = np.concatenate((test, np.zeros((1,768))), axis=0)\n",
    "        exceptions.append(j)\n",
    "\n",
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jew6HE1R_VjY"
   },
   "source": [
    "### You can also read them as we already save them into .npy objects for further uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoGXspcQ_VWO"
   },
   "outputs": [],
   "source": [
    "# new_train = np.load(\"./data/local_train_text_non_processed_BERT.npy\")\n",
    "# new_test = np.load(\"./data/local_test_text_non_processed_BERT.npy\")\n",
    "# new_train_target = np.load(\"./data/target_train_text_non_processed_BERT.npy\")\n",
    "# new_test_target = np.load(\"./data/target_local_test_text_non_processed_BERT.npy\")\n",
    "# test = np.load(\"./data/BERT_test_text_non_processed_76_133_289_304_349_437_525.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_BRGuS85vPI"
   },
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQmn5qNJ4wqH"
   },
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint('./data/weights.hdf5', monitor=\"val_loss\", mode=\"min\", verbose=True, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsH7Yvkb6CE5"
   },
   "outputs": [],
   "source": [
    "def make_model(embed_size=768, loss='categorical_crossentropy', do_rate=0.3, activ=\"relu\"):\n",
    "    inp    = Input(shape=(embed_size,))\n",
    "\n",
    "    # dense  = Dense(512, activation=activ)(inp)\n",
    "\n",
    "    output = Dense(8, activation=\"softmax\")(inp)\n",
    "    \n",
    "    model_ = Model(inputs=inp, outputs=output)\n",
    "    model_.compile(loss=loss, optimizer=Adam(lr=0.0001))\n",
    "    return model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QuYExps4wmv"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 1000\n",
    "do_rate=0.1\n",
    "activ = \"tanh\"\n",
    "NN_clf = make_model(do_rate=do_rate, activ= activ)\n",
    "NN_clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRqODV2S6PPt"
   },
   "source": [
    "### Fitting the classifier on : the local train data and validating on the local test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuX1d7mk4wlL"
   },
   "outputs": [],
   "source": [
    "mapping_labels = {lab:code for code, lab in enumerate(np.unique(new_train_target))}\n",
    "y_train_coded = [mapping_labels[lab] for lab in new_train_target]\n",
    "y_val_coded = [mapping_labels[lab] for lab in new_test_target]\n",
    "history = NN_clf.fit(new_train, to_categorical(y_train_coded), batch_size=batch_size, epochs=epochs, \n",
    "                    validation_data=[new_test, to_categorical(y_val_coded)], \n",
    "                     callbacks=[\n",
    "                                # reduce_lr,\n",
    "                                checkpoints ]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhOJXduv67Iq"
   },
   "source": [
    "### Refitting the model using the hyperparameters found during the previous step (validation) on the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mVMAC-K_4wh-"
   },
   "outputs": [],
   "source": [
    "whole_train = np.concatenate((new_train, new_test),axis=0)\n",
    "whole_train_target = np.concatenate((new_train_target, new_test_target))\n",
    "\n",
    "epochs = 450\n",
    "\n",
    "NN_clf = make_model(do_rate=do_rate, activ= activ)\n",
    "\n",
    "mapping_labels = {lab:code for code, lab in enumerate(np.unique(whole_train_target))}\n",
    "y_train_coded = [mapping_labels[lab] for lab in whole_train_target]\n",
    "\n",
    "history = NN_clf.fit(x=whole_train,\n",
    "                     y=to_categorical(y_train_coded), \n",
    "                     batch_size=batch_size, \n",
    "                     epochs=epochs, \n",
    "                     callbacks=[\n",
    "                                checkpoints\n",
    "                                ]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9HiI0J87VvT"
   },
   "source": [
    "### Saving the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Chv5tzU77Y7s"
   },
   "outputs": [],
   "source": [
    "NN_clf.save(\"./data/model_wrap_up_20_HK_Approach3_rawtext_NN_CLF.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lXCbQoER7JJg"
   },
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXvykI-Y7Iwr"
   },
   "outputs": [],
   "source": [
    "predictions = NN_clf.predict(test)\n",
    "write_submission(\"./data/wrap_up_20_HK_Approach3_rawtext_NN_CLF.csv\", \n",
    "                 list(test_hosts[\"index\"]), \n",
    "                 model_classes_list=list(np.array(train_hosts[[\"class\",\"class_codes\"]].drop_duplicates().sort_values(by='class_codes'))[:,0]), \n",
    "                 predicted_probas=predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DxJ0q7BcyEw6",
    "XXnqoWGlyBiQ",
    "RGYRXb1SMY3r",
    "VtR0EBnEhtKk"
   ],
   "name": "Copie de Alteg_Houcine.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
